import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import os

# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# 1. å®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œ
class CNNDigitClassifier(nn.Module):
    def __init__(self):
        super(CNNDigitClassifier, self).__init__()
        
        # å·ç§¯å±‚
        self.conv_layers = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 28x28x32
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.Conv2d(32, 32, kernel_size=3, padding=1), # 28x28x32
            nn.ReLU(),
            nn.MaxPool2d(2),  # 14x14x32
            nn.Dropout2d(0.25),
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 14x14x64
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.Conv2d(64, 64, kernel_size=3, padding=1), # 14x14x64
            nn.ReLU(),
            nn.MaxPool2d(2),  # 7x7x64
            nn.Dropout2d(0.25),
        )
        
        # å…¨è¿æ¥å±‚
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(7 * 7 * 64, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.5),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 10)
        )
        
    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

# 2. æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½ï¼ˆå‡å°æ•°æ®é›†ï¼‰
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# ä¸‹è½½å¹¶åŠ è½½æ•°æ®
full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# å‡å°æ•°æ®é›†å¤§å°ä»¥åŠ å¿«è®­ç»ƒ
train_size = 15000  # ä»60000å‡å°‘åˆ°15000
test_size = 2500    # ä»10000å‡å°‘åˆ°2500

print(f"ä½¿ç”¨è®­ç»ƒæ ·æœ¬: {train_size}, æµ‹è¯•æ ·æœ¬: {test_size}")

# åˆ›å»ºå­é›†
train_indices = list(range(train_size))
test_indices = list(range(test_size))

train_dataset = Subset(full_train_dataset, train_indices)
test_dataset = Subset(full_test_dataset, test_indices)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)

# 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = CNNDigitClassifier().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")

# 4. è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=8):
    model.train()
    train_losses = []
    train_accuracies = []
    
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        
        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (data, target) in enumerate(progress_bar):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{optimizer.param_groups[0]["lr"]:.6f}'
            })
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc)
        
        print(f'Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')
        
        # æ¯2ä¸ªepochæµ‹è¯•ä¸€æ¬¡
        if (epoch + 1) % 2 == 0:
            test_model(model, test_loader, verbose=False)
    
    return train_losses, train_accuracies

# 5. æµ‹è¯•å‡½æ•°
def test_model(model, test_loader, verbose=True):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    test_loss /= len(test_loader)
    test_acc = 100. * correct / total
    
    if verbose:
        print(f'Final Test Results - Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%')
    else:
        print(f'  Test Accuracy: {test_acc:.2f}%')
    
    return test_loss, test_acc

# 6. å¼€å§‹è®­ç»ƒ
print("å¼€å§‹è®­ç»ƒCNNæ¨¡å‹...")
print("æ³¨æ„ï¼šä½¿ç”¨äº†è¾ƒå°çš„æ•°æ®é›†ï¼Œè®­ç»ƒæ—¶é—´çº¦1-2åˆ†é’Ÿ")
train_losses, train_accuracies = train_model(model, train_loader, criterion, optimizer, scheduler, epochs=8)

# 7. æµ‹è¯•æ¨¡å‹
print("\\næœ€ç»ˆæµ‹è¯•ç»“æœï¼š")
test_loss, test_acc = test_model(model, test_loader)

# 8. ä¿å­˜æ¨¡å‹
os.makedirs('./models', exist_ok=True)
torch.save(model.state_dict(), './models/cnn_digit_classifier.pth')
print("\\nCNNæ¨¡å‹å·²ä¿å­˜åˆ° './models/cnn_digit_classifier.pth'")

# 9. ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹
plt.figure(figsize=(15, 5))

# æŸå¤±æ›²çº¿
plt.subplot(1, 3, 1)
plt.plot(train_losses, 'b-', linewidth=2, label='Training Loss')
plt.title('CNN Training Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# å‡†ç¡®ç‡æ›²çº¿
plt.subplot(1, 3, 2)
plt.plot(train_accuracies, 'r-', linewidth=2, label='Training Accuracy')
plt.title('CNN Training Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True, alpha=0.3)

# æ¨¡å‹æ¶æ„å¯è§†åŒ–
plt.subplot(1, 3, 3)
# ç®€å•çš„æ¶æ„å›¾
layers = ['Input\\n28x28x1', 'Conv1\\n28x28x32', 'Conv2\\n14x14x32', 
          'Conv3\\n14x14x64', 'Conv4\\n7x7x64', 'FC\\n512', 'Output\\n10']
y_pos = np.arange(len(layers))
plt.barh(y_pos, [1]*len(layers), color=['lightblue', 'lightgreen', 'lightgreen', 
                                       'lightcoral', 'lightcoral', 'lightyellow', 'lightpink'])
plt.yticks(y_pos, layers)
plt.xlabel('Layer Depth')
plt.title('CNN Architecture', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

plt.tight_layout()
plt.savefig('./models/cnn_training_progress.png', dpi=300, bbox_inches='tight')
plt.show()

# 10. æ˜¾ç¤ºä¸€äº›æµ‹è¯•æ ·æœ¬çš„é¢„æµ‹ç»“æœ
def show_cnn_predictions(model, test_loader, num_samples=8):
    model.eval()
    fig, axes = plt.subplots(2, 4, figsize=(12, 6))
    axes = axes.ravel()
    
    with torch.no_grad():
        for i, (data, target) in enumerate(test_loader):
            if i >= num_samples:
                break
                
            data, target = data.to(device), target.to(device)
            output = model(data[:num_samples])
            probabilities = torch.softmax(output, dim=1)
            _, predicted = torch.max(output, 1)
            
            for j in range(min(num_samples, len(data))):
                img = data[j].cpu().squeeze()
                true_label = target[j].cpu().item()
                pred_label = predicted[j].cpu().item()
                confidence = probabilities[j][pred_label].cpu().item()
                
                axes[j].imshow(img, cmap='gray')
                color = 'green' if true_label == pred_label else 'red'
                axes[j].set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2%}', 
                                color=color, fontsize=10)
                axes[j].axis('off')
            break
    
    plt.suptitle('CNN Model Predictions', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('./models/cnn_prediction_samples.png', dpi=300, bbox_inches='tight')
    plt.show()

print("\\næ˜¾ç¤ºCNNé¢„æµ‹æ ·æœ¬...")
show_cnn_predictions(model, test_loader)

# 11. æ¨¡å‹å¯¹æ¯”ä¿¡æ¯
print("\\n" + "="*60)
print("ğŸ¯ CNNæ¨¡å‹è®­ç»ƒå®Œæˆ!")
print("="*60)
print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
print(f"âš¡ è®­ç»ƒè½®æ¬¡: 8 epochs")
print(f"ğŸ“¦ æ•°æ®é›†å¤§å°: {train_size:,} è®­ç»ƒæ ·æœ¬, {test_size:,} æµ‹è¯•æ ·æœ¬")
print(f"ğŸ”§ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"ğŸ’¾ æ¨¡å‹æ–‡ä»¶: ./models/cnn_digit_classifier.pth")
print("="*60)
print("ç°åœ¨å¯ä»¥è¿è¡Œäº¤äº’å¼æ¼”ç¤º:")
print("python interactive_cnn_demo.py")
print("="*60)
